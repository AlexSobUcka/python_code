In real-to-sim applications, the goal is to align the simulation
model of the robot and the simulated environment so that
the simulation outputs match the real, observed sensor data as
closely as possible. To do this, the application needs to create a
desired robot state (joint positions for robot arms and twists for
mobile robots) and send the same goal to the real and simulated
robots. It then compares the signals from the real and simulated
sensors, e.g. the joint states of a robot arm or the odometry of a
mobile robot.
Periodically, the application performs a learning process the
goal of which is to determine a set of simulation parameters that
results in identical or nearly identical behavior of the simulated
and real robots. A real-to-sim application therefore attempts to
minimize the error by reconfiguring the parameters of the simulated
system. The architecture of such an application is shown
in Fig. 2.
Since the goal of the described learning process is error
minimization, the real-to-sim process can be performed using
any optimization method. A good optimization criterion could
be any standard metric, e.g. the root mean square error between
normalized sensor data of the simulated robot and the
real robot. For complex applications, the optimization process
must be able to distinguish between the errors whose root is the
robot model and those whose root is the environment model.
For such applications, advanced optimization methods such as
reinforcement learning are recommended. The learning problem
can be posed in such a way that the agent is rewarded based
on the similarity of the behavior of the real and the simulated
robot, while the reward function can also provide incentives for
changing either the robot model or the environment.
The goal of sim-to-real applications is to enable the transfer
of a control policy learned in a simulated environment to
the real environment. Policy learning in simulation for complex
tasks is usually implemented using reinforcement learning [3].
Especially for real applications, it is important to correctly identify
the learning problem and use mechanisms that improve the
robustness of the learned policies, such as domain randomization.
The architecture of a sim-to-real application is shown in
Fig. 3.